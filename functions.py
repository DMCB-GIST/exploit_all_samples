# -*- coding: utf-8 -*-
"""
Created on Thu Nov  5 05:01:12 2020

@author: Anonymity
"""

import pickle
import torch
import pyarrow
import datasets
import numpy as np
import torch.nn.functional as F

from sklearn.metrics import accuracy_score
from sklearn.metrics import log_loss, f1_score, accuracy_score

def dump_all(all_data, data_path):
	with open(data_path, 'wb') as handle:
		pickle.dump(all_data, handle)

def load_all(data_path):
	with open(data_path, 'rb') as handle:
		load_all = pickle.loads(handle.read())
	return load_all

class InputExample(object):
	def __init__(self, guid, text_a, text_b=None, label=None):
		self.guid = guid
		self.text_a = text_a
		self.text_b = text_b
		self.label = label

def text_reader(txt_path):
	samples = []
	f = open(txt_path,'r')
	for line in f.readlines():
		label,sent = line.strip().split('\t')
		samples.append((label,sent))
	f.close()
	return samples

def samples2examples(samples):
	examples = []
	for sample_i,sample in enumerate(samples):
		guid = 'data-'+str(sample_i)
		example = InputExample(guid=guid, text_a=sample[1], text_b='', label=sample[0])
		examples.append(example)
	return examples

def divide_samples(samples,si,use_rate,trn_rate,swap=False): 
	si = int(si%(1./use_rate-1))
	unit_len = int(use_rate*len(samples))
	indices = [i for i in range(si*unit_len,(si+1)*unit_len)]
	
	unit_samples = select_by_index(samples,indices)
	tst_samples = select_by_index(samples,indices,reverse=True)
	
	trn_len = int(trn_rate*unit_len+0.5)
	if swap:
		trn_samples = unit_samples[unit_len-trn_len:]
		dev_samples = unit_samples[:unit_len-trn_len]
	else:
		trn_samples = unit_samples[:trn_len]
		dev_samples = unit_samples[trn_len:]
	return trn_samples,dev_samples,tst_samples

def select_by_index(lst,indices,reverse=False):
	selected = []
	if reverse:
		for i in range(len(lst)):
			if i in indices:
				continue
			selected.append(lst[i])
	else:
		for i in indices:
			selected.append(lst[i])
	return selected

def example2arrow(examples, label_map):
	feat_names = ['text1', 'labels']
	texts1,labels = [],[]
	for i in range(len(examples)):
		texts1.append( examples[i].text_a )
		labels.append( label_map[examples[i].label] )

	arrow_table = pyarrow.Table.from_arrays([texts1,labels], names=feat_names)
	return datasets.arrow_dataset.Dataset(arrow_table)

def arrow2batch(arrow, indices, tokenizer, max_length, device):
	batch_arrow = arrow[indices]

	batch_pt = tokenizer(text=batch_arrow['text1'], truncation=True,
						 padding='longest', max_length=max_length, return_tensors='pt').to(device)
	batch_pt_labels = torch.from_numpy(np.array(batch_arrow['labels'])).to(device)

	return (batch_pt, batch_pt_labels)

def eval_accuracy(tst_arrow, tst_batch, model, tokenizer, max_length, classifier, device, return_probs=False):
	if type(tst_arrow) is dict:
		tst_arrow = datasets.arrow_dataset.Dataset.from_dict(tst_arrow)
	num_labels = classifier.weight.shape[0] 

	model.eval()
	with torch.no_grad():
		tst_probs, tst_preds, tst_golds = [],[],[]
		for ti in range(  int(np.ceil(len(tst_arrow)/tst_batch))  ):
			tst_bat_indices = np.arange(ti*tst_batch,  min( (ti+1)*tst_batch, len(tst_arrow))  )
			batch_pt, batch_pt_labels = arrow2batch(tst_arrow, tst_bat_indices, tokenizer, max_length, device)

			model.eval()
			bat_vectors = model(**batch_pt).pooler_output
			tst_bat_logits = classifier(bat_vectors)
			tst_probs += F.softmax(tst_bat_logits, dim=1).detach().cpu().numpy().tolist()
			tst_preds += tst_bat_logits.argmax(1).detach().cpu().numpy().tolist()
			tst_golds += batch_pt_labels.detach().cpu().numpy().tolist()

	if return_probs:
		return (np.array(tst_golds), np.array(tst_probs))
	else:
		return (accuracy_score(tst_golds, tst_preds), log_loss(tst_golds, tst_probs, labels=np.arange(num_labels)))

def eval_metrics(label_ids,data_probs,bins):
	_,num_labels = np.shape(data_probs)
	data_acc = accuracy_score(np.argmax(data_probs,1),label_ids)
	data_loss = log_loss(label_ids, data_probs, labels=np.arange(num_labels)) # cross_entropy(data_probs,label_ids)
	prob_per_bin,crrt_per_bin = calc_calibration(data_probs,label_ids,bins=bins)
	data_ece,data_oe = calc_ece_and_oe(prob_per_bin,crrt_per_bin)
	return data_acc,data_loss,data_ece,data_oe

def calc_calibration(data_probs,label_ids,bins=10):
	_,num_labels = np.shape(data_probs)
	data_len = len(label_ids) #

	base_prob = 1.0/num_labels
	rmn_prob = 1.0-base_prob
	unit_prob = rmn_prob/bins

	prob_per_bin = {bi:[] for bi in range(bins)}
	crrt_per_bin = {bi:[] for bi in range(bins)}
	for di in range(data_len):
		pred_i = np.argmax(data_probs[di])
		pred_prob = data_probs[di][pred_i]
		true_i = label_ids[di]
		iscorrect = float(true_i == pred_i)

		for bi in range(bins):
			if pred_prob > bi*unit_prob+base_prob and pred_prob <= (bi+1)*unit_prob+base_prob:
				prob_per_bin[bi].append(pred_prob)
				crrt_per_bin[bi].append(iscorrect)
				break
	return prob_per_bin,crrt_per_bin

def calc_ece_and_oe(prob_per_bin,crrt_per_bin):
	ece = 0.0
	oe = 0.0
	data_size = 0
	for bi in range(len(prob_per_bin)):
		size_bm = len(prob_per_bin[bi])
		data_size += size_bm
		if size_bm==0:
			continue
		conf_bm = np.mean(prob_per_bin[bi])
		acc_bm = np.mean(crrt_per_bin[bi])
		ece += size_bm*abs(acc_bm-conf_bm)
		oe += size_bm*(conf_bm*max(conf_bm-acc_bm,0.0))
	ece /= data_size
	oe /= data_size
	return ece, oe
